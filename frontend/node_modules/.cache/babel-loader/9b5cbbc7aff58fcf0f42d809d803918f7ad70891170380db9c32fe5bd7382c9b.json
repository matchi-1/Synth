{"ast":null,"code":"// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nimport { APIResource } from \"../../../resource.mjs\";\nimport { isRequestOptions } from \"../../../core.mjs\";\nimport { sleep } from \"../../../core.mjs\";\nimport { allSettledWithThrow } from \"../../../lib/Util.mjs\";\nimport { VectorStoreFilesPage } from \"./files.mjs\";\nexport class FileBatches extends APIResource {\n  /**\n   * Create a vector store file batch.\n   */\n  create(vectorStoreId, body, options) {\n    return this._client.post(`/vector_stores/${vectorStoreId}/file_batches`, {\n      body,\n      ...options,\n      headers: {\n        'OpenAI-Beta': 'assistants=v2',\n        ...options?.headers\n      }\n    });\n  }\n  /**\n   * Retrieves a vector store file batch.\n   */\n  retrieve(vectorStoreId, batchId, options) {\n    return this._client.get(`/vector_stores/${vectorStoreId}/file_batches/${batchId}`, {\n      ...options,\n      headers: {\n        'OpenAI-Beta': 'assistants=v2',\n        ...options?.headers\n      }\n    });\n  }\n  /**\n   * Cancel a vector store file batch. This attempts to cancel the processing of\n   * files in this batch as soon as possible.\n   */\n  cancel(vectorStoreId, batchId, options) {\n    return this._client.post(`/vector_stores/${vectorStoreId}/file_batches/${batchId}/cancel`, {\n      ...options,\n      headers: {\n        'OpenAI-Beta': 'assistants=v2',\n        ...options?.headers\n      }\n    });\n  }\n  /**\n   * Create a vector store batch and poll until all files have been processed.\n   */\n  async createAndPoll(vectorStoreId, body, options) {\n    const batch = await this.create(vectorStoreId, body);\n    return await this.poll(vectorStoreId, batch.id, options);\n  }\n  listFiles(vectorStoreId, batchId, query = {}, options) {\n    if (isRequestOptions(query)) {\n      return this.listFiles(vectorStoreId, batchId, {}, query);\n    }\n    return this._client.getAPIList(`/vector_stores/${vectorStoreId}/file_batches/${batchId}/files`, VectorStoreFilesPage, {\n      query,\n      ...options,\n      headers: {\n        'OpenAI-Beta': 'assistants=v2',\n        ...options?.headers\n      }\n    });\n  }\n  /**\n   * Wait for the given file batch to be processed.\n   *\n   * Note: this will return even if one of the files failed to process, you need to\n   * check batch.file_counts.failed_count to handle this case.\n   */\n  async poll(vectorStoreId, batchId, options) {\n    const headers = {\n      ...options?.headers,\n      'X-Stainless-Poll-Helper': 'true'\n    };\n    if (options?.pollIntervalMs) {\n      headers['X-Stainless-Custom-Poll-Interval'] = options.pollIntervalMs.toString();\n    }\n    while (true) {\n      const {\n        data: batch,\n        response\n      } = await this.retrieve(vectorStoreId, batchId, {\n        ...options,\n        headers\n      }).withResponse();\n      switch (batch.status) {\n        case 'in_progress':\n          let sleepInterval = 5000;\n          if (options?.pollIntervalMs) {\n            sleepInterval = options.pollIntervalMs;\n          } else {\n            const headerInterval = response.headers.get('openai-poll-after-ms');\n            if (headerInterval) {\n              const headerIntervalMs = parseInt(headerInterval);\n              if (!isNaN(headerIntervalMs)) {\n                sleepInterval = headerIntervalMs;\n              }\n            }\n          }\n          await sleep(sleepInterval);\n          break;\n        case 'failed':\n        case 'cancelled':\n        case 'completed':\n          return batch;\n      }\n    }\n  }\n  /**\n   * Uploads the given files concurrently and then creates a vector store file batch.\n   *\n   * The concurrency limit is configurable using the `maxConcurrency` parameter.\n   */\n  async uploadAndPoll(vectorStoreId, {\n    files,\n    fileIds = []\n  }, options) {\n    if (files == null || files.length == 0) {\n      throw new Error(`No \\`files\\` provided to process. If you've already uploaded files you should use \\`.createAndPoll()\\` instead`);\n    }\n    const configuredConcurrency = options?.maxConcurrency ?? 5;\n    // We cap the number of workers at the number of files (so we don't start any unnecessary workers)\n    const concurrencyLimit = Math.min(configuredConcurrency, files.length);\n    const client = this._client;\n    const fileIterator = files.values();\n    const allFileIds = [...fileIds];\n    // This code is based on this design. The libraries don't accommodate our environment limits.\n    // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\n    async function processFiles(iterator) {\n      for (let item of iterator) {\n        const fileObj = await client.files.create({\n          file: item,\n          purpose: 'assistants'\n        }, options);\n        allFileIds.push(fileObj.id);\n      }\n    }\n    // Start workers to process results\n    const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\n    // Wait for all processing to complete.\n    await allSettledWithThrow(workers);\n    return await this.createAndPoll(vectorStoreId, {\n      file_ids: allFileIds\n    });\n  }\n}\n(function (FileBatches) {})(FileBatches || (FileBatches = {}));\nexport { VectorStoreFilesPage };\n//# sourceMappingURL=file-batches.mjs.map","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}